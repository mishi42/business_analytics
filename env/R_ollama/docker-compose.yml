services:
  rstudio:
    #image: rocker/tidyverse
    container_name: rstudio_llm
    build :
       context: .
       dockerfile: ./rstudio.dockerfile
    environment:
      - USERNAME=rstudio
      - PASSWORD=rstudio1234
      - TZ=Asia/Tokyo
      - MLFLOW_TRACKING_URI='http://mlflow:5000'
    ports:
      - 8787:8787
      - 9991:9991
      - 9992:9992
      - 9993:9993
      - 9994:9994
      - 9995:9995
    volumes:
      - ../../work/:/mnt
      - ollama_env:/work/ollama_env/
      - shinydata:/srv/shiny-server/
      - shinylog:/srv/shinylog/
    #working_dir: /work
    tty: true
    restart: always      
    depends_on:
      - ollama
      - mlflow
      - selenium
      - mattermost
      - db
    networks:
      - r_ollama_net

  ollama:
    # image: ollama/ollama
    container_name: ollama
    build :
       context: .
       dockerfile: ./ollama.dockerfile
    mem_limit: 8g
    memswap_limit: 8g
    ports:
      - 11434:11434
    tty: true
    restart: unless-stopped
    volumes:
      - ollama_env:/root/.ollama
    networks:
      - r_ollama_net

  shiny_app:
    container_name: shiny_app
    build:
      context: . 
      dockerfile: ./shiny.dockerfile    
    ports:
      - 3838:3838
    mem_limit: 4g
    memswap_limit: 4g
    tty: true
    restart: unless-stopped
    volumes:
      - shinydata:/srv/shiny-server
      - shinylog:/var/log/shiny-server/ 
    depends_on:
      - ollama
      - rstudio
    networks:
      - r_ollama_net
  
  mlflow:
    container_name: mlflow
    build:
      #context: ./mlflow-tracking
      context: . 
      dockerfile: ./mlflow.dockerfile
    mem_limit: 1g
    memswap_limit: 1g
    expose:
      - "5000"
    ports:
      - 5000:5000
    command: mlflow server --backend-store-uri /work/mlruns --default-artifact-root /work/mlruns --host 0.0.0.0 --port 5000
    tty: true
    restart: always
    networks:
      - r_ollama_net

  #open-webui:
  #  container_name: open_webui
  #  image: ghcr.io/open-webui/open-webui:main
  #  ports:
  #    - "3000:8080"
  #  volumes:
  #    - open-webui:/app/backend/data
  #  extra_hosts:
  #    - "host.docker.internal:host-gateway"
  #  restart: always
  #  networks:
  #    - r_ollama_net

  #llm_studio:
    #  container_name: llm_studio
    #image: h2oairelease/h2oai-llmstudio-app:latest
    #ports:
    #  - 10101 :10101 

  selenium:
    container_name: selenium_chrome
    build :
       context: .
       dockerfile: ./selenium.dockerfile
    mem_limit: 1g
    memswap_limit: 1g
    ports:
      - 4444:4444 
      - 7900:7900
    restart: always
    tty: true
    networks:
      - r_ollama_net

  mattermost:
    container_name: mattermost
    image: mattermost/mattermost-team-edition
    mem_limit: 1g
    memswap_limit: 1g
    ports:
      - 8765:8065 
    environment:
      - MM_SQLSETTINGS_DRIVERNAME=postgres
      - MM_SQLSETTINGS_DATASOURCE=postgres://mmuser:password@db:5432/mattermost?sslmode=disable
      - TZ=Asia/Tokyo
    #volumes:
    #  - ./volumes/app/mattermost/:/mattermost/
    tty: true
    restart: unless-stopped
    depends_on:
      - db
    networks:
      - r_ollama_net

  db:
    container_name: postgres
    image: postgres:16-alpine
    mem_limit: 1g
    memswap_limit: 1g
    environment:
      - POSTGRES_USER=mmuser
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=mattermost
      - TZ=Asia/Tokyo
    #volumes:
    #  - ./volumes/db:/var/lib/postgresql/data
    tty: true
    restart: unless-stopped
    networks:
      - r_ollama_net

  portainer:
    image: portainer/portainer-ce:alpine
    container_name: portainer
    mem_limit: 1g
    memswap_limit: 1g
    ports:
      - 9443:9443
      - 9000:9000
    restart: unless-stopped
    #volumes:
    #  - /var/run/docker.sock:/var/run/docker.sock

networks:
  r_ollama_net:
    driver: bridge

volumes:
  ollama:
  shinydata:
  shinylog:
  ollama_env: 
  #open-webui:
